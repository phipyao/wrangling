{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
   "metadata": {
    "id": "13ad028b-72b7-43ed-aa78-96fd4e518040"
   },
   "source": [
    "# Assignment: Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7b15217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5735a4d4-8be8-433a-a351-70eb8002e632",
   "metadata": {
    "id": "5735a4d4-8be8-433a-a351-70eb8002e632"
   },
   "source": [
    "**Q1.** Open the \"tidy_data.pdf\" document in the repo, which is a paper called Tidy Data by Hadley Wickham.\n",
    "\n",
    "  1. Read the abstract. What is this paper about?\n",
    "\n",
    "  >The paper is about the process of data cleaning, specifically about tidy datasets, where each variable is a column, each observation is a row, and each unit group is a table.\n",
    "\n",
    "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
    "\n",
    "  >The tidy data standard intends to make data analysis easier by providing a standardized way of organizing values within a dataset.\n",
    "\n",
    "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, itâ€™s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
    "\n",
    "  >The first sentence means that tidy data sets are all similar in one way, but messy data sets are all uniquely different from one another.\n",
    "\n",
    "  >The second sentence means that defining what the rows and columns are for datasets is easy, but defining the observational units is not as obvious.\n",
    "\n",
    "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
    "\n",
    "  > Values are numeric or categorical/strings. Variables are a collection of values that measure the same thing. An obersvation is a collection of values that measures the observational unit.\n",
    "\n",
    "  5. How is \"Tidy Data\" defined in section 2.3?\n",
    "\n",
    "  > Variables are columns, obervations are rows, and observational units are tables.\n",
    "\n",
    "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
    "\n",
    "  >1. column headers are values, not variables.\n",
    "  >2. multiple variables are stored in one column.\n",
    "  >3. variables are stored in both rows and columns.\n",
    "  >4. multiple observational units are stored in one table.\n",
    "  >5. a single observational unit is stored in multiple tables.\n",
    "\n",
    "  > In table 4, the columns all define income and frequency, which should be separated into individual rows.\n",
    "  > Melting a dataset is the process of converting column-value variables into rows.\n",
    "\n",
    "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
    "\n",
    "  > Table 11 is messy because there are days as columns, but they should be values instead.\n",
    "  > Table 12 is molten because each tmax and tmin measurement lies on the same day, so it does not need a separate observational row. It is also tidy because all the entries contain unique attributes and has no variable names as columns.\n",
    "\n",
    "  8. Read Section 6. What is the \"chicken-and-egg\" problem with focusing on tidy data? What does Wickham hope happens in the future with further work on the subject of data wrangling?\n",
    "\n",
    "  > The chicken-and-egg problem is that people will only build tools with tidy data, and so those tools will only be useful with tidy data. Wickham hopes that there will be new frameworks and tools that expand off tidy data rather than only relying on it as the sole source of data representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072",
   "metadata": {
    "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072"
   },
   "source": [
    "**Q2.** This question provides some practice cleaning variables which have common problems.\n",
    "1. Numeric variable: For `./data/airbnb_hw.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n",
    "\n",
    "    >0 missing values.\n",
    "\n",
    "2. Categorical variable: For the `./data/sharks.csv` data covered in the lecture, clean the \"Type\" variable as well as you can, and explain the choices you make.\n",
    "\n",
    "\n",
    "    >I combined all the incidents that took place on a watercraft or boat to be just under the boat variable in order to consolidate the different types. I then changed all the missing or unknown data to be null values to clean the data.\n",
    "\n",
    "\n",
    "3. Dummy variable: For the pretrial data covered in the lecture, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`.\n",
    "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)\n",
    "\n",
    "    >I set the sentence length to 0 when the sentence type was 4 because those cases had charges that were pending, dismissed, or deferred. I set the sentence length to null values when the sentence type was 9 becuase those were not applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "432e153d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing:  0\n"
     ]
    }
   ],
   "source": [
    "#cleaning for q2.1\n",
    "df = pd.read_csv('./data/airbnb_hw.csv', low_memory = False)\n",
    "price = df['Price']\n",
    "#price.value_counts()\n",
    "price = price.str.replace(',', '')\n",
    "price = pd.to_numeric(price, errors = 'coerce')\n",
    "\n",
    "print('missing: ', sum(price.isnull())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fe2f590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Type\n",
       "Unprovoked    4716\n",
       "Provoked       593\n",
       "Boat           583\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaning for q2.2\n",
    "df = pd.read_csv('./data/sharks.csv', low_memory = False)\n",
    "type = df['Type']\n",
    "#type.value_counts()\n",
    "type = type.replace(['Sea Disaster', 'Watercraft', 'Boating', 'Boatomg'], 'Boat') # All boats\n",
    "type = type.replace(['Invalid', 'Questionable', 'Unconfirmed', 'Unverified', 'Under investigation'], np.nan) # All null\n",
    "\n",
    "type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c55e956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning for q2.3\n",
    "url = 'http://www.vcsc.virginia.gov/pretrialdataproject/October%202017%20Cohort_Virginia%20Pretrial%20Data%20Project_Deidentified%20FINAL%20Update_10272021.csv'\n",
    "df = pd.read_csv(url,low_memory=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76444953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhetherDefendantWasReleasedPretrial\n",
       "1.0    19154\n",
       "0.0     3801\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "release = df['WhetherDefendantWasReleasedPretrial']\n",
    "# release.value_counts()\n",
    "release = release.replace(9, np.nan)\n",
    "\n",
    "# sum(release.isnull()) \n",
    "release.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81d31e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9053\n",
      "274\n"
     ]
    }
   ],
   "source": [
    "#cleaning for q2.4\n",
    "length = df['ImposedSentenceAllChargeInContactEvent']\n",
    "type = df['SentenceTypeAllChargesAtConvictionInContactEvent']\n",
    "\n",
    "# print(length.value_counts())\n",
    "# print(type.value_counts())\n",
    "\n",
    "length = pd.to_numeric(length, errors = 'coerce')\n",
    "print(sum(length.isnull()))\n",
    "\n",
    "length = length.mask(type == 4, 0) # pending, dismissed, or deferred charges\n",
    "length = length.mask(type == 9, np.nan) # not applicable\n",
    "print(sum(length.isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5",
   "metadata": {
    "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5"
   },
   "source": [
    "**Q3.** Many important datasets contain a race variable, typically limited to a handful of values often including Black, White, Asian, Latino, and Indigenous. This question looks at data gathering efforts on this variable by the U.S. Federal government.\n",
    "\n",
    "1. How did the most recent US Census gather data on race?\n",
    "2. Why do we gather these data? What role do these kinds of data play in politics and society? Why does data quality matter?\n",
    "3. Please provide a constructive criticism of how the Census was conducted: What was done well? What do you think was missing? How should future large scale surveys be adjusted to best reflect the diversity of the population? Could some of the Census' good practices be adopted more widely to gather richer and more useful data?\n",
    "4. How did the Census gather data on sex and gender? Please provide a similar constructive criticism of their practices.\n",
    "5. When it comes to cleaning data, what concerns do you have about protected characteristics like sex, gender, sexual identity, or race? What challenges can you imagine arising when there are missing values? What good or bad practices might people adopt, and why?\n",
    "6. Suppose someone invented an algorithm to impute values for protected characteristics like race, gender, sex, or sexuality. What kinds of concerns would you have?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
